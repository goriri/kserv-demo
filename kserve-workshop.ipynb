{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d11cbf4a-079a-4ad9-811c-d8eda19597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ebe569e-a34a-486a-bb15-f371e11c1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2b05ba-3836-4ebd-9f8f-42fcb40dfc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "073ca098-8458-4243-9aa4-bd39a3146858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('variables.env', override=True)  # take environment variables from variables.env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d60d5f9d-6853-43a1-8e99-9fd6f63b877c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing install-eksctl.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile install-eksctl.sh\n",
    "# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`\n",
    "ARCH=amd64\n",
    "PLATFORM=$(uname -s)_$ARCH\n",
    "\n",
    "curl -sLO \"https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz\"\n",
    "\n",
    "# (Optional) Verify checksum\n",
    "curl -sL \"https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt\" | grep $PLATFORM | sha256sum --check\n",
    "\n",
    "tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz\n",
    "\n",
    "sudo mv /tmp/eksctl /usr/local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f77393c9-7ee5-4260-8a7e-9e1b94726082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eksctl_Linux_amd64.tar.gz: OK\n"
     ]
    }
   ],
   "source": [
    "!chmod +x install-eksctl.sh\n",
    "!./install-eksctl.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1eb10627-0e1d-4b43-9216-5c5ecd523aca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create-eks-cluster.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile create-eks-cluster.sh\n",
    "curl -fsSL https://raw.githubusercontent.com/aws/karpenter-provider-aws/v\"${KARPENTER_VERSION}\"/website/content/en/preview/getting-started/getting-started-with-karpenter/cloudformation.yaml  > \"${TEMPOUT}\" \\\n",
    "&& aws cloudformation deploy \\\n",
    "  --stack-name \"Karpenter-${CLUSTER_NAME}\" \\\n",
    "  --template-file \"${TEMPOUT}\" \\\n",
    "  --capabilities CAPABILITY_NAMED_IAM \\\n",
    "  --parameter-overrides \"ClusterName=${CLUSTER_NAME}\"\n",
    "\n",
    "eksctl create cluster -f - <<EOF\n",
    "---\n",
    "apiVersion: eksctl.io/v1alpha5\n",
    "kind: ClusterConfig\n",
    "metadata:\n",
    "  name: ${CLUSTER_NAME}\n",
    "  region: ${AWS_DEFAULT_REGION}\n",
    "  version: \"${K8S_VERSION}\"\n",
    "  tags:\n",
    "    karpenter.sh/discovery: ${CLUSTER_NAME}\n",
    "\n",
    "iam:\n",
    "  withOIDC: true\n",
    "  podIdentityAssociations:\n",
    "  - namespace: \"${KARPENTER_NAMESPACE}\"\n",
    "    serviceAccountName: karpenter\n",
    "    roleName: ${CLUSTER_NAME}-karpenter\n",
    "    permissionPolicyARNs:\n",
    "    - arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}\n",
    "\n",
    "iamIdentityMappings:\n",
    "- arn: \"arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}\"\n",
    "  username: system:node:{{EC2PrivateDNSName}}\n",
    "  groups:\n",
    "  - system:bootstrappers\n",
    "  - system:nodes\n",
    "  ## If you intend to run Windows workloads, the kube-proxy group should be specified.\n",
    "  # For more information, see https://github.com/aws/karpenter/issues/5099.\n",
    "  # - eks:kube-proxy-windows\n",
    "\n",
    "managedNodeGroups:\n",
    "- instanceType: m5.large\n",
    "  amiFamily: AmazonLinux2\n",
    "  name: ${CLUSTER_NAME}-ng\n",
    "  desiredCapacity: 2\n",
    "  minSize: 1\n",
    "  maxSize: 10\n",
    "\n",
    "addons:\n",
    "- name: eks-pod-identity-agent\n",
    "EOF\n",
    "\n",
    "export CLUSTER_ENDPOINT=\"$(aws eks describe-cluster --name \"${CLUSTER_NAME}\" --query \"cluster.endpoint\" --output text)\"\n",
    "export KARPENTER_IAM_ROLE_ARN=\"arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter\"\n",
    "\n",
    "echo \"${CLUSTER_ENDPOINT} ${KARPENTER_IAM_ROLE_ARN}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1379b9-c9a4-4897-b76c-8aff1ba6b386",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waiting for changeset to be created..\n",
      "\n",
      "No changes to deploy. Stack Karpenter-kserve-demo is up to date\n",
      "\u001b[36m2024-06-07 06:13:33 [ℹ]  eksctl version 0.181.0\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  using region us-west-2\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  setting availability zones to [us-west-2a us-west-2b us-west-2c]\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  subnets for us-west-2a - public:192.168.0.0/19 private:192.168.96.0/19\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  subnets for us-west-2c - public:192.168.64.0/19 private:192.168.160.0/19\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  nodegroup \"kserve-demo-ng\" will use \"\" [AmazonLinux2/1.29]\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  using Kubernetes version 1.29\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  creating EKS cluster \"kserve-demo\" in \"us-west-2\" region with managed nodes\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  1 nodegroup (kserve-demo-ng) was included (based on the include/exclude rules)\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s)\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  will create a CloudFormation stack for cluster itself and 1 managed nodegroup stack(s)\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=kserve-demo'\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"kserve-demo\" in \"us-west-2\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  CloudWatch logging will not be enabled for cluster \"kserve-demo\" in \"us-west-2\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-west-2 --cluster=kserve-demo'\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  \n",
      "2 sequential tasks: { create cluster control plane \"kserve-demo\", \n",
      "    2 sequential sub-tasks: { \n",
      "        6 sequential sub-tasks: { \n",
      "            wait for control plane to become ready,\n",
      "            associate IAM OIDC provider,\n",
      "            2 sequential sub-tasks: { \n",
      "                create IAM role for serviceaccount \"kube-system/aws-node\",\n",
      "                create serviceaccount \"kube-system/aws-node\",\n",
      "            },\n",
      "            restart daemonset \"kube-system/aws-node\",\n",
      "            create IAM identity mappings,\n",
      "            1 task: { create addons },\n",
      "        },\n",
      "        create managed nodegroup \"kserve-demo-ng\",\n",
      "    } \n",
      "}\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  building cluster stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:13:33 [ℹ]  deploying stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:14:03 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:14:33 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:15:33 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:16:33 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:17:33 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:18:34 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:19:34 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m\u001b[36m2024-06-07 06:20:34 [ℹ]  waiting for CloudFormation stack \"eksctl-kserve-demo-cluster\"\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!chmod +x create-eks-cluster.sh\n",
    "!./create-eks-cluster.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db20fa84-4a0d-4713-921a-259c8f7574ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "CLUSTER_ENDPOINT=\"$(aws eks describe-cluster --name \"${CLUSTER_NAME}\" --query \"cluster.endpoint\" --output text)\"\n",
    "KARPENTER_IAM_ROLE_ARN=\"arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter\"\n",
    "cat > variables2.env << EOF\n",
    "CLUSTER_ENDPOINT=${AWS_DEFAULT_REGION}\n",
    "KARPENTER_IAM_ROLE_ARN=${AWS_ACCESS_KEY_ID}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0523954-8b64-4e0b-82a2-263e3700e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('variables2.env', override=True)  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d4cf2e9-f6b8-4622-adde-8e414f3c9188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Role\": {\n",
      "        \"Path\": \"/aws-service-role/spot.amazonaws.com/\",\n",
      "        \"RoleName\": \"AWSServiceRoleForEC2Spot\",\n",
      "        \"RoleId\": \"AROAURO4PORKWYWP7PPEO\",\n",
      "        \"Arn\": \"arn:aws:iam::312384386133:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\",\n",
      "        \"CreateDate\": \"2024-06-07T08:08:07+00:00\",\n",
      "        \"AssumeRolePolicyDocument\": {\n",
      "            \"Version\": \"2012-10-17\",\n",
      "            \"Statement\": [\n",
      "                {\n",
      "                    \"Action\": [\n",
      "                        \"sts:AssumeRole\"\n",
      "                    ],\n",
      "                    \"Effect\": \"Allow\",\n",
      "                    \"Principal\": {\n",
      "                        \"Service\": [\n",
      "                            \"spot.amazonaws.com\"\n",
      "                        ]\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws iam create-service-linked-role --aws-service-name spot.amazonaws.com || true\n",
    "# If the role has already been successfully created, you will see:\n",
    "# An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b71bf77-eec8-4c6b-968a-48a0c28e4bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://get.helm.sh/helm-v3.15.1-linux-amd64.tar.gz\n",
      "Verifying checksum... Done.\n",
      "Preparing to install helm into /usr/local/bin\n",
      "helm installed into /usr/local/bin/helm\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n",
    "chmod 700 get_helm.sh\n",
    "./get_helm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5763a35-12a6-40cf-b02d-8b6b548b0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: not logged in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"karpenter\" does not exist. Installing it now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pulled: public.ecr.aws/karpenter/karpenter:0.36.2\n",
      "Digest: sha256:1b3b57830f7368b8934651001c1919a284af5a470b751c9d1a4204fed122cb3c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: karpenter\n",
      "LAST DEPLOYED: Fri Jun  7 08:13:18 2024\n",
      "NAMESPACE: kube-system\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Logout of helm registry to perform an unauthenticated pull against the public ECR\n",
    "helm registry logout public.ecr.aws\n",
    "\n",
    "helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version \"${KARPENTER_VERSION}\" --namespace \"${KARPENTER_NAMESPACE}\" --create-namespace \\\n",
    "  --set \"settings.clusterName=${CLUSTER_NAME}\" \\\n",
    "  --set \"settings.interruptionQueue=${CLUSTER_NAME}\" \\\n",
    "  --set controller.resources.requests.cpu=1 \\\n",
    "  --set controller.resources.requests.memory=1Gi \\\n",
    "  --set controller.resources.limits.cpu=1 \\\n",
    "  --set controller.resources.limits.memory=1Gi \\\n",
    "  --wait\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a4f7ba8-1c2d-4895-ac99-304156f2e690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting karpenter-cpu-node-pool.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile karpenter-cpu-node-pool.yaml\n",
    "apiVersion: karpenter.sh/v1beta1\n",
    "kind: NodePool\n",
    "metadata:\n",
    "  name: default\n",
    "spec:\n",
    "  # Template section that describes how to template out NodeClaim resources that Karpenter will provision\n",
    "  # Karpenter will consider this template to be the minimum requirements needed to provision a Node using this NodePool\n",
    "  # It will overlay this NodePool with Pods that need to schedule to further constrain the NodeClaims\n",
    "  # Karpenter will provision to launch new Nodes for the cluster\n",
    "  template:\n",
    "    spec:\n",
    "      # References the Cloud Provider's NodeClass resource, see your cloud provider specific documentation\n",
    "      nodeClassRef:\n",
    "        apiVersion: karpenter.k8s.aws/v1beta1\n",
    "        kind: EC2NodeClass\n",
    "        name: default\n",
    "\n",
    "      # Requirements that constrain the parameters of provisioned nodes.\n",
    "      # These requirements are combined with pod.spec.topologySpreadConstraints, pod.spec.affinity.nodeAffinity, pod.spec.affinity.podAffinity, and pod.spec.nodeSelector rules.\n",
    "      # Operators { In, NotIn, Exists, DoesNotExist, Gt, and Lt } are supported.\n",
    "      # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators\n",
    "      requirements:\n",
    "        - key: \"karpenter.k8s.aws/instance-category\"\n",
    "          operator: In\n",
    "          values: [\"c\", \"m\", \"r\"]\n",
    "        - key: \"karpenter.k8s.aws/instance-cpu\"\n",
    "          operator: In\n",
    "          values: [\"4\", \"8\", \"16\", \"32\"]\n",
    "        - key: \"karpenter.k8s.aws/instance-hypervisor\"\n",
    "          operator: In\n",
    "          values: [\"nitro\"]\n",
    "        - key: \"karpenter.k8s.aws/instance-generation\"\n",
    "          operator: Gt\n",
    "          values: [\"2\"]\n",
    "        # - key: \"topology.kubernetes.io/zone\"\n",
    "        #   operator: In\n",
    "        #   values: [\"us-west-2a\", \"us-west-2b\"]\n",
    "        - key: \"kubernetes.io/arch\"\n",
    "          operator: In\n",
    "          values: [\"arm64\", \"amd64\"]\n",
    "        - key: \"karpenter.sh/capacity-type\"\n",
    "          operator: In\n",
    "          values: [\"spot\", \"on-demand\"]\n",
    "        - key: \"karpenter.k8s.aws/instance-cpu-manufacturer\"\n",
    "          operator: In\n",
    "          values: [\"intel\"]\n",
    "\n",
    "  # Disruption section which describes the ways in which Karpenter can disrupt and replace Nodes\n",
    "  # Configuration in this section constrains how aggressive Karpenter can be with performing operations\n",
    "  # like rolling Nodes due to them hitting their maximum lifetime (expiry) or scaling down nodes to reduce cluster cost\n",
    "  disruption:\n",
    "    # Describes which types of Nodes Karpenter should consider for consolidation\n",
    "    # If using 'WhenUnderutilized', Karpenter will consider all nodes for consolidation and attempt to remove or replace Nodes when it discovers that the Node is underutilized and could be changed to reduce cost\n",
    "    # If using `WhenEmpty`, Karpenter will only consider nodes for consolidation that contain no workload pods\n",
    "    consolidationPolicy: WhenEmpty\n",
    "\n",
    "    # The amount of time Karpenter should wait after discovering a consolidation decision\n",
    "    # This value can currently only be set when the consolidationPolicy is 'WhenEmpty'\n",
    "    # You can choose to disable consolidation entirely by setting the string value 'Never' here\n",
    "    consolidateAfter: 30s\n",
    "\n",
    "    # The amount of time a Node can live on the cluster before being removed\n",
    "    # Avoiding long-running Nodes helps to reduce security vulnerabilities as well as to reduce the chance of issues that can plague Nodes with long uptimes such as file fragmentation or memory leaks from system processes\n",
    "    # You can choose to disable expiration entirely by setting the string value 'Never' here\n",
    "    expireAfter: 720h\n",
    "\n",
    "  # Resource limits constrain the total size of the pool.\n",
    "  # Limits prevent Karpenter from creating new instances once the limit is exceeded.\n",
    "  limits:\n",
    "    cpu: \"1000\"\n",
    "    memory: 1000Gi\n",
    "\n",
    "  # Priority given to the NodePool when the scheduler considers which NodePool\n",
    "  # to select. Higher weights indicate higher priority when comparing NodePools.\n",
    "  # Specifying no weight is equivalent to specifying a weight of 0.\n",
    "  weight: 10\n",
    "---\n",
    "apiVersion: karpenter.k8s.aws/v1beta1\n",
    "kind: EC2NodeClass\n",
    "metadata:\n",
    "  name: default\n",
    "spec:\n",
    "  # Required, resolves a default ami and userdata\n",
    "  amiFamily: AL2\n",
    "  role: KarpenterNodeRole-kserve-demo\n",
    "  # Required, discovers subnets to attach to instances\n",
    "  # Each term in the array of subnetSelectorTerms is ORed together\n",
    "  # Within a single term, all conditions are ANDed\n",
    "  subnetSelectorTerms:\n",
    "    # Select on any subnet that has the \"karpenter.sh/discovery: ${CLUSTER_NAME}\"\n",
    "    # AND the \"environment: test\" tag OR any subnet with ID \"subnet-09fa4a0a8f233a921\"\n",
    "    - tags:\n",
    "        karpenter.sh/discovery: kserve-demo\n",
    "\n",
    "  # Required, discovers security groups to attach to instances\n",
    "  # Each term in the array of securityGroupSelectorTerms is ORed together\n",
    "  # Within a single term, all conditions are ANDed\n",
    "  securityGroupSelectorTerms:\n",
    "    # Select on any security group that has both the \"karpenter.sh/discovery: ${CLUSTER_NAME}\" tag\n",
    "    # AND the \"environment: test\" tag OR any security group with the \"my-security-group\" name\n",
    "    # OR any security group with ID \"sg-063d7acfb4b06c82c\"\n",
    "    - tags:\n",
    "        karpenter.sh/discovery: kserve-demo\n",
    "\n",
    "  blockDeviceMappings:\n",
    "    - deviceName: /dev/xvda\n",
    "      ebs:\n",
    "        volumeSize: 200Gi\n",
    "        volumeType: gp3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34a86ae6-7f5b-405d-a770-6cc68f23585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing karpenter-gpu-node-pool.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile karpenter-gpu-node-pool.yaml\n",
    "apiVersion: karpenter.sh/v1beta1\n",
    "kind: NodePool\n",
    "metadata:\n",
    "  name: gpu\n",
    "spec:\n",
    "  disruption:\n",
    "    consolidationPolicy: WhenUnderutilized\n",
    "  template:\n",
    "    spec:\n",
    "      nodeClassRef: \n",
    "        apiVersion: karpenter.k8s.aws/v1beta1\n",
    "        kind: EC2NodeClass\n",
    "        name: gpu\n",
    "      requirements:\n",
    "      - key: \"karpenter.k8s.aws/instance-category\"\n",
    "        operator: In\n",
    "        values: [\"g\"]\n",
    "      - key: \"karpenter.k8s.aws/instance-gpu-name\"\n",
    "        operator: In\n",
    "        values: [\"a10g\"]\n",
    "      - key: \"karpenter.k8s.aws/instance-gpu-count\"\n",
    "        operator: Gt\n",
    "        values: [\"0\"]\n",
    "      - key: \"karpenter.k8s.aws/instance-gpu-count\"\n",
    "        operator: Lt\n",
    "        values: [\"10\"]\n",
    "      - key: \"karpenter.sh/capacity-type\"\n",
    "        operator: In\n",
    "        values: [\"spot\", \"on-demand\"]\n",
    "      - key: \"kubernetes.io/arch\"\n",
    "        operator: In\n",
    "        values: [\"amd64\"]\n",
    "      - key: \"kubernetes.io/os\"\n",
    "        operator: In\n",
    "        values: [\"linux\"]\n",
    "      - key: karpenter.k8s.aws/instance-local-nvme\n",
    "        operator: Gt\n",
    "        values: [\"100\"]\n",
    "  limits:\n",
    "    nvidia.com/gpu: \"10\"\n",
    "  weight: 1\n",
    "---\n",
    "apiVersion: karpenter.k8s.aws/v1beta1\n",
    "kind: EC2NodeClass\n",
    "metadata:\n",
    "  name: gpu\n",
    "spec:\n",
    "  # Required, resolves a default ami and userdata\n",
    "  amiFamily: Ubuntu\n",
    "  role: KarpenterNodeRole-kserve-demo\n",
    "  instanceStorePolicy: RAID0\n",
    "  # Required, discovers subnets to attach to instances\n",
    "  # Each term in the array of subnetSelectorTerms is ORed together\n",
    "  # Within a single term, all conditions are ANDed\n",
    "  subnetSelectorTerms:\n",
    "    # Select on any subnet that has the \"karpenter.sh/discovery: ${CLUSTER_NAME}\"\n",
    "    # AND the \"environment: test\" tag OR any subnet with ID \"subnet-09fa4a0a8f233a921\"\n",
    "    - tags:\n",
    "        karpenter.sh/discovery: kserve-demo\n",
    "\n",
    "  # Required, discovers security groups to attach to instances\n",
    "  # Each term in the array of securityGroupSelectorTerms is ORed together\n",
    "  # Within a single term, all conditions are ANDed\n",
    "  securityGroupSelectorTerms:\n",
    "    # Select on any security group that has both the \"karpenter.sh/discovery: ${CLUSTER_NAME}\" tag\n",
    "    # AND the \"environment: test\" tag OR any security group with the \"my-security-group\" name\n",
    "    # OR any security group with ID \"sg-063d7acfb4b06c82c\"\n",
    "    - tags:\n",
    "        karpenter.sh/discovery: kserve-demo\n",
    "\n",
    "  amiSelectorTerms:\n",
    "  - name: amazon-eks-gpu-node-1.29-v20240531\n",
    "\n",
    "  blockDeviceMappings:\n",
    "    - deviceName: /dev/xvda\n",
    "      ebs:\n",
    "        volumeSize: 200Gi\n",
    "        volumeType: gp3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cc1708f-f67b-47a7-980f-4acbadb680c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   138  100   138    0     0   1068      0 --:--:-- --:--:-- --:--:--  1069\n",
      "100 47.4M  100 47.4M    0     0  78.2M      0 --:--:-- --:--:-- --:--:-- 78.2M\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl\n",
    "!chmod +x kubectl\n",
    "!mkdir -p ~/.local/bin\n",
    "!mv ./kubectl ~/.local/bin/kubectl\n",
    "%env PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/sagemaker-user/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa8e76fd-beab-4e92-8172-12ffc7dfd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (AlreadyExists): error when creating \"https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/deployments/static/nvidia-device-plugin.yml\": daemonsets.apps \"nvidia-device-plugin-daemonset\" already exists\n",
      "nodepool.karpenter.sh/default created\n",
      "nodepool.karpenter.sh/gpu created\n",
      "ec2nodeclass.karpenter.k8s.aws/gpu created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/deployments/static/nvidia-device-plugin.yml\n",
    "!kubectl apply -f karpenter-cpu-node-pool.yaml\n",
    "# if you are not running in workshop environment, please uncomment the following line\n",
    "# !kubectl apply -f karpenter-gpu-node-pool.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75d55559-e758-4994-a169-7f2985cd5c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/sagemaker-user/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# add tag kubernetes.io/cluster/kserve-demo=owned to ClusterSharedNodeSecurityGroup to create load balancer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad10fce2-4a9f-4a8e-b54b-cf0f7edacfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libjq1 libonig5\n",
      "The following NEW packages will be installed:\n",
      "  jq libjq1 libonig5\n",
      "0 upgraded, 3 newly installed, 0 to remove and 9 not upgraded.\n",
      "Need to get 357 kB of archives.\n",
      "After this operation, 1087 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libonig5 amd64 6.9.7.1-2build1 [172 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjq1 amd64 1.6-2.1ubuntu3 [133 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 jq amd64 1.6-2.1ubuntu3 [52.5 kB]\n",
      "Fetched 357 kB in 1s (641 kB/s)m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libonig5:amd64.\n",
      "(Reading database ... 13795 files and directories currently installed.)\n",
      "Preparing to unpack .../libonig5_6.9.7.1-2build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libjq1:amd64.\n",
      "Preparing to unpack .../libjq1_1.6-2.1ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package jq.\n",
      "Preparing to unpack .../jq_1.6-2.1ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking jq (1.6-2.1ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Setting up libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up jq (1.6-2.1ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Processing triggers for libc-bin (2.35-0ubuntu3.7) ...\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "\u001b[1;31mE: \u001b[0mUnable to locate package cosign\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install jq -y\n",
    "!sudo apt install cosign -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9561471-8154-41ce-b6a0-27daefb0766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O -L \"https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64\"\n",
    "!mv cosign-linux-amd64 ~/.local/bin/cosign\n",
    "!chmod +x ~/.local/bin/cosign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c256c6b7-31d2-442c-bcce-53ebbdc92682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ISTIO_VERSION=1.21.3\n",
      "\n",
      "Downloading istioctl-1.21.3 from https://github.com/istio/istio/releases/download/1.21.3/istioctl-1.21.3-linux-amd64.tar.gz ...\n",
      "istioctl-1.21.3-linux-amd64.tar.gz download complete!\n",
      "\n",
      "Add the istioctl to your path with:\n",
      "  export PATH=$HOME/.istioctl/bin:$PATH \n",
      "\n",
      "Begin the Istio pre-installation check by running:\n",
      "\t istioctl x precheck \n",
      "\n",
      "Need more information? Visit https://istio.io/docs/reference/commands/istioctl/ \n",
      "env: PATH=/home/sagemaker-user/.istioctl/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/sagemaker-user/.local/bin\n"
     ]
    }
   ],
   "source": [
    "%env ISTIO_VERSION=1.21.3\n",
    "!curl -sL https://istio.io/downloadIstioctl | sh -\n",
    "%env PATH=/home/sagemaker-user/.istioctl/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/sagemaker-user/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f19c7114-a67b-470f-a452-a48c80d89396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "- Processing resources for Istio core.\n",
      "✔ Istio core installed\n",
      "- Processing resources for Istiod.\n",
      "- Processing resources for Istiod. Waiting for Deployment/istio-system/istiod\n",
      "✘ Istiod encountered an error: failed to wait for resource: resources not ready after 5m0s: context deadline exceeded\n",
      "  \n",
      "- Processing resources for Ingress gateways.\n",
      "- Processing resources for Ingress gateways. Waiting for Deployment/istio-system/istio-ingressgat...\n",
      "✘ Ingress gateways encountered an error: failed to wait for resource: resources not ready after 5m0s: context deadline exceeded\n",
      "  Deployment/istio-system/istio-ingressgateway (container failed to start: ContainerCreating: )\n",
      "- Pruning removed resourcesError: failed to install manifests: errors occurred during operation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/knative-serving not labeled\n",
      "1 Istio control planes detected, checking --revision \"default\" only\n",
      "✘ Deployment: istio-ingressgateway.istio-system: Istio installation failed, incomplete or does not match \"generated from default\": waiting for deployment \"istio-ingressgateway\" rollout to finish: 0 of 1 updated replicas are available\n",
      "✘ Deployment: istiod.istio-system: Istio installation failed, incomplete or does not match \"generated from default\": deployment \"istiod\" exceeded its progress deadline\n",
      "✔ Service: istio-ingressgateway.istio-system checked successfully\n",
      "✔ Service: istiod.istio-system checked successfully\n",
      "✔ ConfigMap: istio.istio-system checked successfully\n",
      "✔ ConfigMap: istio-sidecar-injector.istio-system checked successfully\n",
      "✔ Pod: istio-ingressgateway-59d6b88bc9-mtm6b.istio-system checked successfully\n",
      "✔ Pod: istiod-865d6658f8-9btsw.istio-system checked successfully\n",
      "✔ ServiceAccount: istio-ingressgateway-service-account.istio-system checked successfully\n",
      "✔ ServiceAccount: istio-reader-service-account.istio-system checked successfully\n",
      "✔ ServiceAccount: istiod.istio-system checked successfully\n",
      "✔ RoleBinding: istio-ingressgateway-sds.istio-system checked successfully\n",
      "✔ RoleBinding: istiod.istio-system checked successfully\n",
      "✔ Role: istio-ingressgateway-sds.istio-system checked successfully\n",
      "✔ Role: istiod.istio-system checked successfully\n",
      "✔ PodDisruptionBudget: istio-ingressgateway.istio-system checked successfully\n",
      "✔ PodDisruptionBudget: istiod.istio-system checked successfully\n",
      "✔ HorizontalPodAutoscaler: istio-ingressgateway.istio-system checked successfully\n",
      "✔ HorizontalPodAutoscaler: istiod.istio-system checked successfully\n",
      "✔ MutatingWebhookConfiguration: istio-sidecar-injector.istio-system checked successfully\n",
      "✔ ValidatingWebhookConfiguration: istio-validator-istio-system.istio-system checked successfully\n",
      "✔ ClusterRole: istio-reader-clusterrole-istio-system.istio-system checked successfully\n",
      "✔ ClusterRole: istiod-clusterrole-istio-system.istio-system checked successfully\n",
      "✔ ClusterRole: istiod-gateway-controller-istio-system.istio-system checked successfully\n",
      "✔ ClusterRoleBinding: istio-reader-clusterrole-istio-system.istio-system checked successfully\n",
      "✔ ClusterRoleBinding: istiod-clusterrole-istio-system.istio-system checked successfully\n",
      "✔ ClusterRoleBinding: istiod-gateway-controller-istio-system.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: authorizationpolicies.security.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: destinationrules.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: envoyfilters.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: gateways.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: peerauthentications.security.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: proxyconfigs.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: requestauthentications.security.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: serviceentries.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: sidecars.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: telemetries.telemetry.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: virtualservices.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: wasmplugins.extensions.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: workloadentries.networking.istio.io.istio-system checked successfully\n",
      "✔ CustomResourceDefinition: workloadgroups.networking.istio.io.istio-system checked successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: [Istio installation failed, incomplete or does not match \"generated from default\": waiting for deployment \"istio-ingressgateway\" rollout to finish: 0 of 1 updated replicas are available, Istio installation failed, incomplete or does not match \"generated from default\": deployment \"istiod\" exceeded its progress deadline]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterrole.rbac.authorization.k8s.io/knative-serving-istio unchanged\n",
      "gateway.networking.istio.io/knative-ingress-gateway created\n",
      "gateway.networking.istio.io/knative-local-gateway created\n",
      "service/knative-local-gateway created\n",
      "configmap/config-istio unchanged\n",
      "peerauthentication.security.istio.io/webhook created\n",
      "peerauthentication.security.istio.io/net-istio-webhook created\n",
      "deployment.apps/net-istio-controller unchanged\n",
      "deployment.apps/net-istio-webhook unchanged\n",
      "secret/net-istio-webhook-certs unchanged\n",
      "service/net-istio-webhook unchanged\n",
      "mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.istio.networking.internal.knative.dev unchanged\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.istio.networking.internal.knative.dev unchanged\n",
      "certificate.networking.internal.knative.dev/routing-serving-certs created\n",
      "namespace/cert-manager unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io unchanged\n",
      "serviceaccount/cert-manager-cainjector unchanged\n",
      "serviceaccount/cert-manager unchanged\n",
      "serviceaccount/cert-manager-webhook unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-cluster-view unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-view unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-edit unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews configured\n",
      "role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged\n",
      "role.rbac.authorization.k8s.io/cert-manager:leaderelection unchanged\n",
      "role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving unchanged\n",
      "rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged\n",
      "rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection configured\n",
      "rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving configured\n",
      "service/cert-manager unchanged\n",
      "service/cert-manager-webhook unchanged\n",
      "deployment.apps/cert-manager-cainjector unchanged\n",
      "deployment.apps/cert-manager unchanged\n",
      "deployment.apps/cert-manager-webhook unchanged\n",
      "mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured\n",
      "namespace/kserve unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/clusterservingruntimes.serving.kserve.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/clusterstoragecontainers.serving.kserve.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/inferencegraphs.serving.kserve.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/inferenceservices.serving.kserve.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/servingruntimes.serving.kserve.io unchanged\n",
      "customresourcedefinition.apiextensions.k8s.io/trainedmodels.serving.kserve.io unchanged\n",
      "serviceaccount/kserve-controller-manager unchanged\n",
      "role.rbac.authorization.k8s.io/kserve-leader-election-role unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/kserve-manager-role unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/kserve-proxy-role unchanged\n",
      "rolebinding.rbac.authorization.k8s.io/kserve-leader-election-rolebinding unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/kserve-manager-rolebinding unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/kserve-proxy-rolebinding unchanged\n",
      "configmap/inferenceservice-config unchanged\n",
      "secret/kserve-webhook-server-secret unchanged\n",
      "service/kserve-controller-manager-metrics-service unchanged\n",
      "service/kserve-controller-manager-service unchanged\n",
      "service/kserve-webhook-server-service unchanged\n",
      "deployment.apps/kserve-controller-manager unchanged\n",
      "certificate.cert-manager.io/serving-cert created\n",
      "issuer.cert-manager.io/selfsigned-issuer created\n",
      "mutatingwebhookconfiguration.admissionregistration.k8s.io/inferenceservice.serving.kserve.io configured\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/clusterservingruntime.serving.kserve.io configured\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/inferencegraph.serving.kserve.io configured\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/inferenceservice.serving.kserve.io configured\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/servingruntime.serving.kserve.io configured\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/trainedmodel.serving.kserve.io configured\n",
      "clusterstoragecontainer.serving.kserve.io/default unchanged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n",
      "Error from server (InternalError): error when creating \"https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\": Internal error occurred: failed calling webhook \"clusterservingruntime.kserve-webhook-server.validator\": failed to call webhook: Post \"https://kserve-webhook-server-service.kserve.svc:443/validate-serving-kserve-io-v1alpha1-clusterservingruntime?timeout=10s\": no endpoints available for service \"kserve-webhook-server-service\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/config-domain patched (no change)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -sSL https://github.com/knative/serving/releases/download/knative-v1.13.1/serving-core.yaml \\\n",
    "  | grep 'gcr.io/' | awk '{print $2}' | sort | uniq \\\n",
    "  | xargs -n 1 \\\n",
    "    cosign verify -o text \\\n",
    "      --certificate-identity=signer@knative-releases.iam.gserviceaccount.com \\\n",
    "      --certificate-oidc-issuer=https://accounts.google.com\n",
    "kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.13.1/serving-crds.yaml\n",
    "kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.13.1/serving-core.yaml\n",
    "istioctl install -y\n",
    "kubectl label namespace knative-serving istio-injection=enabled\n",
    "# kubectl apply -f knative-serving-peer-auth.yaml\n",
    "istioctl verify-install\n",
    "kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.13.1/net-istio.yaml\n",
    "kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.yaml\n",
    "kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.13.0/kserve.yaml\n",
    "kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\n",
    "#Since you are using Knative 1.8, there is a change that it defaults the domain to svc.cluster.local which is not exposed on ingress, if you want to run the curl command outside of the kube cluster you would need to configure the external domain.\n",
    "kubectl patch cm config-domain --patch '{\"data\":{\"example.com\":\"\"}}' -n knative-serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9e9a40f-6109-410e-9a32-0988e5628059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"kubernetes-dashboard\" has been added to your repositories\n",
      "Release \"kubernetes-dashboard\" does not exist. Installing it now.\n",
      "NAME: kubernetes-dashboard\n",
      "LAST DEPLOYED: Fri Jun  7 09:06:00 2024\n",
      "NAMESPACE: kubernetes-dashboard\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "*************************************************************************************************\n",
      "*** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ***\n",
      "*************************************************************************************************\n",
      "\n",
      "Congratulations! You have just installed Kubernetes Dashboard in your cluster.\n",
      "\n",
      "To access Dashboard run:\n",
      "  kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443\n",
      "\n",
      "NOTE: In case port-forward command does not work, make sure that kong service name is correct.\n",
      "      Check the services in Kubernetes Dashboard namespace using:\n",
      "        kubectl -n kubernetes-dashboard get svc\n",
      "\n",
      "Dashboard will be available at:\n",
      "  https://localhost:8443\n",
      "serviceaccount/dashboard-admin-sa created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/dashboard-admin-sa created\n",
      "secret/dashboard-admin-sa-secret created\n",
      "Name:         dashboard-admin-sa-secret\n",
      "Namespace:    default\n",
      "Labels:       <none>\n",
      "Annotations:  kubernetes.io/service-account.name: dashboard-admin-sa\n",
      "              kubernetes.io/service-account.uid: 81f02f91-65c5-437a-959b-949718f1b2c3\n",
      "\n",
      "Type:  kubernetes.io/service-account-token\n",
      "\n",
      "Data\n",
      "====\n",
      "ca.crt:     1107 bytes\n",
      "namespace:  7 bytes\n",
      "token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlB5SHB6Vy1zMEpLMHpTMnhkM29xdnRUMktHTmxaQXpZRjIzWUpjMFNfdVEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRhc2hib2FyZC1hZG1pbi1zYS1zZWNyZXQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluLXNhIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODFmMDJmOTEtNjVjNS00MzdhLTk1OWItOTQ5NzE4ZjFiMmMzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6ZGFzaGJvYXJkLWFkbWluLXNhIn0.INoxnqIYoaO9-33WSQfsV0A3YDksOYhw-kQDkn1YVzXEVJQopgSO_eUUp07hzhM6YaBGUEtHVcnQrIUezS0PX0AXrKwVD437vX6-tmvEUPYe6JppzMljVEMeo2mPt4yRjqlFxXA9RW10AhCqN44TV0ryuXQJCrjy63haDzKytyLF6jG-HBh0kx4xbRT--J6_8fO8N6te8AHrFLVI5HNHhb-XQ6zsWomx2SEt5nvmvYSiIqxLfn5YN2pTl80KqKnBUQCyuw4Ux9VJ0daxjYk49-Gg9jb09yEszPhJ_GQu4eum0217t7RQwH0yjBzHDbO-3qJDOIQdMdalNprs8tq9XA\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Add kubernetes-dashboard repository\n",
    "helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\n",
    "# Deploy a Helm Release named \"kubernetes-dashboard\" using the kubernetes-dashboard chart\n",
    "helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard\n",
    "\n",
    "kubectl create serviceaccount dashboard-admin-sa\n",
    "kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: dashboard-admin-sa-secret\n",
    "  annotations:\n",
    "    kubernetes.io/service-account.name: dashboard-admin-sa\n",
    "type: kubernetes.io/service-account-token\n",
    "EOF\n",
    "kubectl describe secret dashboard-admin-sa-secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1d9b5-7fe4-4b0b-8337-0ed5772eca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kubectl edit svc/kubernetes-dashboard-kong-proxy -n kubernetes-dashboard\n",
    "#ClusterIP => LoadBalancer and then ctrl+X Y Enter\n",
    "#Use the CLB addr (https://xxxx) to visit to the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a968babd-d16d-45a8-9cdb-364038970316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"sklearn-iris\" deleted\n",
      "inferenceservice.serving.kserve.io/sklearn-iris created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl create namespace kserve-test\n",
    "# is the following required?\n",
    "# kubectl label namespace kserve-test istio-injection=enabled\n",
    "kubectl apply -n kserve-test -f - <<EOF\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"sklearn-iris\"\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: sklearn\n",
    "      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "340271b2-3600-402f-a181-23d40610b68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           URL                                           READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION            AGE\n",
      "sklearn-iris   http://sklearn-iris.kserve-test.example.com   True           100                              sklearn-iris-predictor-00001   2m18s\n"
     ]
    }
   ],
   "source": [
    "#wait until READY = TRUE\n",
    "!kubectl get inferenceservices sklearn-iris -n kserve-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "11e77495-3c6d-4ed9-a8f1-c95433f0a326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing iris-input.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris-input.json\n",
    "{\n",
    "  \"instances\": [\n",
    "    [6.8,  2.8,  4.8,  1.4],\n",
    "    [6.0,  3.4,  4.5,  1.6]\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "001e9a65-47f4-4917-8b5b-000cc78eed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a8526a93672a448229e70fde870ffeef-89339606.us-west-2.elb.amazonaws.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 52.26.94.246:80...\n",
      "* Connected to a8526a93672a448229e70fde870ffeef-89339606.us-west-2.elb.amazonaws.com (52.26.94.246) port 80 (#0)\n",
      "> POST /v1/models/sklearn-iris:predict HTTP/1.1\n",
      "> Host: sklearn-iris.kserve-test.example.com\n",
      "> User-Agent: curl/7.81.0\n",
      "> Accept: */*\n",
      "> Content-Type: application/json\n",
      "> Content-Length: 76\n",
      "> \n",
      "} [76 bytes data]\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< content-length: 21\n",
      "< content-type: application/json\n",
      "< date: Fri, 07 Jun 2024 11:08:51 GMT\n",
      "< server: istio-envoy\n",
      "< x-envoy-upstream-service-time: 8\n",
      "< \n",
      "{ [21 bytes data]\n",
      "100    97  100    21  100    76   1095   3962 --:--:-- --:--:-- --:--:--  5105\n",
      "* Connection #0 to host a8526a93672a448229e70fde870ffeef-89339606.us-west-2.elb.amazonaws.com left intact\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[1,1]}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n",
    "export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}')\n",
    "echo ${INGRESS_HOST}:${INGRESS_PORT}\n",
    "SERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris -n kserve-test -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n",
    "curl -v -H \"Host: ${SERVICE_HOSTNAME}\" -H \"Content-Type: application/json\" \"http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/sklearn-iris:predict\" -d @./iris-input.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951ed9e-e59a-42fd-bc82-6856e8458de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist-tensorflow model from gs://kfserving-examples/models/\n",
    "!aws s3 cp --recursive mnist-tensorflow/ s3://YOUR_BUCKET/mnist-tensorflow/\n",
    "# Create the service account + IAM Role via eksctl and then modify the service account\n",
    "!eksctl create iamserviceaccount --name s3-sa --namespace kserve-test --cluster kserve-demo --role-name kserve-demo-s3-access \\\n",
    "    --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --approve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c82335-460b-4cfa-90df-bb556ff9743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile s3-sa.yaml\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: s3-sa\n",
    "  annotations:\n",
    "    eks.amazonaws.com/role-arn: arn:aws:iam::YOUR_ACCOUNT:role/kserve-demo-s3-access # replace with your IAM role ARN\n",
    "    # serving.kserve.io/s3-endpoint: s3.us-west-2.amazonaws.com # replace with your s3 endpoint e.g minio-service.kubeflow:9000\n",
    "    # serving.kserve.io/s3-usehttps: \"1\" # by default 1, if testing with minio you can set to 0\n",
    "    serving.kserve.io/s3-region: \"us-west-2\"\n",
    "    serving.kserve.io/s3-useanoncredential: \"false\" # omitting this is the same as false, if true will ignore provided credential and use anonymous credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2606c-0de6-4f78-845d-3a8714fb670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist-s3.yaml\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"mnist-s3\"\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: s3-sa\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: tensorflow\n",
    "      storageUri: \"s3://YOUR_BUCKET/mnist-tensorflow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4c840-a1c2-4736-bd0f-6f7ae1a380f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f s3-sa.yaml -n kserve-test\n",
    "!kubectl apply -f mnist-s3.yaml -n kserve-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bd976-6a7a-4f0c-acfa-1953ed261a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SERVICE_HOSTNAME=$(kubectl get inferenceservice mnist-s3 -n kserve-test -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n",
    "\n",
    "MODEL_NAME=mnist-s3\n",
    "INPUT_PATH=@./input.json\n",
    "curl -v -H \"Host: ${SERVICE_HOSTNAME}\" -H \"Content-Type: application/json\" \"http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict\" -d $INPUT_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01fad4-f98f-4c39-8f8c-6e89e72808b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following test won't work in the workshop due to no GPU resource availause huggingface cli to download the model weights and then upload s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d1841-23eb-4641-978f-762e1053030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface-llama3.yaml\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: llama3\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: s3-sa\n",
    "    scaleTarget: 5\n",
    "    scaleMetric: concurrency\n",
    "    # batcher:\n",
    "    #   maxBatchSize: 4\n",
    "    #   maxLatency: 500\n",
    "    model:\n",
    "      storageUri: \"s3://YOUR_BUCKET/Meta-Llama-3-8B\"\n",
    "      modelFormat:\n",
    "        name: huggingface\n",
    "      args:\n",
    "      - --model_name=llama3\n",
    "      - --dtype=bfloat16\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: \"6\"\n",
    "          memory: 24Gi\n",
    "          nvidia.com/gpu: \"1\"\n",
    "          ephemeral-storage: \"40Gi\"\n",
    "        requests:\n",
    "          cpu: \"6\"\n",
    "          memory: 24Gi\n",
    "          nvidia.com/gpu: \"1\"\n",
    "          ephemeral-storage: \"20Gi\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
